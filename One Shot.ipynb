{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a752c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments, AutoConfig)\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch import nn\n",
    "from datasets import load_dataset, concatenate_datasets, Features, Value\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 8):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(8)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = \"finbert\"  # \"finbert\" or \"roberta\"\n",
    "strategy = \"LoRA\" # \"LoRA\" or \"FT\"\n",
    "noise_amth = 20000  # noise amount for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eca6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)     # Remove extra spaces\n",
    "    text = text.strip()                  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "ds_ax_clean = []\n",
    "feats = Features({\"text\": Value(\"string\"), \"label\": Value(\"int64\")})\n",
    "for x in ['emoji', 'emotion', 'hate', 'irony', 'offensive', 'sentiment', 'stance_abortion', 'stance_atheism', 'stance_climate', 'stance_feminist', 'stance_hillary']:\n",
    "  ds = (load_dataset(\"cardiffnlp/tweet_eval\", x)[\"train\"])\n",
    "  ds = ds.remove_columns(\"label\")\n",
    "  ds = ds.map(lambda x: {\"text\" : clean_text(x[\"text\"]), \"label\": 3})\n",
    "  ds = ds.cast(feats)\n",
    "  ds_ax_clean.append(ds)\n",
    "ds_ax_clean = concatenate_datasets(ds_ax_clean)\n",
    "\n",
    "\n",
    "sampled = ds_ax_clean.shuffle(seed=8).select(range(min(noise_amth, len(ds_ax_clean))))\n",
    "\n",
    "ds_fin = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "ds_fin = ds_fin.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "full_ds = concatenate_datasets([ds_fin[\"train\"], ds_fin[\"validation\"], sampled])\n",
    "\n",
    "\n",
    "split_test = full_ds.train_test_split(train_size=0.90, seed=8)\n",
    "train_full_ds = split_test[\"train\"]\n",
    "test_ds       = split_test[\"test\"]\n",
    "\n",
    "split_val = train_full_ds.train_test_split(train_size=0.88, seed=8)\n",
    "train_ds = split_val[\"train\"]\n",
    "val_ds   = split_val[\"test\"]\n",
    "\n",
    "print(np.bincount(train_ds[\"label\"], minlength=4))\n",
    "print(np.bincount(val_ds[\"label\"],   minlength=4))\n",
    "print(np.bincount(test_ds[\"label\"],  minlength=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model ==\"finbert\":\n",
    "    MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "elif model == \"roberta\":\n",
    "    MODEL_NAME = \"roberta-base\"\n",
    "OUTDIR = \"out/exp1\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def preprocess(batch):\n",
    "\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "# tokenize\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns= [\"text\"])\n",
    "val_tok   = val_ds.map(preprocess,   batched=True, remove_columns= [\"text\"])\n",
    "test_tok  = test_ds.map(preprocess,  batched=True, remove_columns= [\"text\"])\n",
    "full_train_tok = concatenate_datasets([train_tok , val_tok])\n",
    "\n",
    "\n",
    "for split in (full_train_tok, test_tok):\n",
    "    split.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# use dynamic padding at batch time\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "NUM_LABELS = 4  # since labels are 0,1,2,3\n",
    "counts = Counter(train_full_ds[\"label\"])  \n",
    "\n",
    "\n",
    "total = sum(counts[c] for c in range(NUM_LABELS))\n",
    "inv_freq = [total / (counts[i] if counts[i] > 0 else 1) for i in range(NUM_LABELS)]\n",
    "weights = torch.tensor(inv_freq, dtype=torch.float32)\n",
    "weights = weights / weights.mean()  \n",
    "print(\"class weights (0,1,2,3):\", weights.tolist())\n",
    "weights = weights.to(device)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ada2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(train_full_ds[\"label\"], minlength=NUM_LABELS)\n",
    "w = counts.sum() / np.maximum(counts, 1)\n",
    "w = torch.tensor(w / w.mean(), dtype=torch.float32)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs): \n",
    "        labels = inputs.pop(\"labels\")                 \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.class_weights is None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.10\n",
    "\n",
    "if model == \"finbert\":\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=True,   \n",
    "        force_download=True,            \n",
    "    )\n",
    "elif model == \"roberta\":\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "if strategy == \"LoRA\":\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=[\"query\",\"key\",\"value\",\"dense\"]  \n",
    "    )\n",
    "    base = get_peft_model(base, lora_cfg)\n",
    "    base.print_trainable_parameters()\n",
    "\n",
    "import torch\n",
    "base.to(device)\n",
    "\n",
    "\n",
    "if strategy == \"LoRA\":\n",
    "    LR = 1e-4\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"out/twitter-finance\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=max(32, BATCH_SIZE),\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        seed=8,\n",
    "        fp16=(device == \"cuda\"),\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "elif strategy == \"FT\":\n",
    "    LR = 2e-5\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"out/twitter-finance_fullft\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=max(32, BATCH_SIZE),\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=0.1,                 \n",
    "        logging_steps=50,\n",
    "        seed=8,\n",
    "        fp16=(device == \"cuda\"),\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=base,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_tok,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights = w\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"global_step:\", trainer.state.global_step)\n",
    "val_metrics = trainer.evaluate(val_tok)\n",
    "\n",
    "score = float(val_metrics.get(\"eval_macro_f1\", 0.0))\n",
    "print(score)\n",
    "\n",
    "\n",
    "test_metrics = trainer.evaluate(test_tok)\n",
    "test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0df9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(test_tok)   \n",
    "logits = pred.predictions             \n",
    "y_true = pred.label_ids               \n",
    "\n",
    "# class ids\n",
    "y_pred = logits.argmax(axis=-1)\n",
    "y_true = np.array([int(x) for x in test_ds[\"label\"]], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred)\n",
    "macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(acc, macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fe117",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2,3])\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
