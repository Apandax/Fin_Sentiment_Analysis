{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a752c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments, AutoConfig)\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import re\n",
    "from datasets import load_dataset, concatenate_datasets, Features, Value\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 8):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(8)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = \"roberta\"  # \"finbert\" or \"roberta\"\n",
    "strategy = \"FT\"   # \"LoRA\" or \"FT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eca6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)     # Remove extra spaces\n",
    "    text = text.strip()                  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "ds_ax_clean = []\n",
    "feats = Features({\"text\": Value(\"string\"), \"label\": Value(\"int64\")})\n",
    "for x in ['emoji', 'emotion', 'hate', 'irony', 'offensive', 'sentiment', 'stance_abortion', 'stance_atheism', 'stance_climate', 'stance_feminist', 'stance_hillary']:\n",
    "  ds = (load_dataset(\"cardiffnlp/tweet_eval\", x)[\"train\"])\n",
    "  ds = ds.remove_columns(\"label\")\n",
    "  ds = ds.map(lambda x: {\"text\" : clean_text(x[\"text\"]), \"label\": 3})\n",
    "  ds = ds.cast(feats)\n",
    "  ds_ax_clean.append(ds)\n",
    "ds_ax_clean = concatenate_datasets(ds_ax_clean)\n",
    "\n",
    "\n",
    "sampled_70000 = ds_ax_clean.shuffle(seed=8).select(range(min(70000, len(ds_ax_clean))))\n",
    "\n",
    "ds_fin = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "ds_fin = ds_fin.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "s2_full_ds = concatenate_datasets([ds_fin[\"train\"], ds_fin[\"validation\"], sampled_70000])\n",
    "\n",
    "\n",
    "\n",
    "split_test = s2_full_ds.train_test_split(train_size=0.90, seed=8)\n",
    "s2_train_full_ds = split_test[\"train\"]\n",
    "s2_test_ds       = split_test[\"test\"]\n",
    "\n",
    "split_val = s2_train_full_ds.train_test_split(train_size=0.88, seed=8)\n",
    "s2_train_ds = split_val[\"train\"]\n",
    "s2_val_ds   = split_val[\"test\"]\n",
    "\n",
    "\n",
    "s1_full_ds = s2_full_ds.map(lambda ex: {\"label\": 0 if ex[\"label\"] in (0,1,2) else 1})\n",
    "s1_train_full_ds = s2_train_full_ds.map(lambda ex: {\"label\": 0 if ex[\"label\"] in (0,1,2) else 1})\n",
    "s1_train_ds = s2_train_ds.map(lambda ex: {\"label\": 0 if ex[\"label\"] in (0,1,2) else 1})\n",
    "s1_val_ds   = s2_val_ds.map(lambda ex: {\"label\": 0 if ex[\"label\"] in (0,1,2) else 1})\n",
    "s1_test_ds = s2_test_ds.map(lambda ex: {\"label\": 0 if ex[\"label\"] in (0,1,2) else 1})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model ==\"finbert\":\n",
    "    MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "elif model == \"roberta\":\n",
    "    MODEL_NAME = \"roberta-base\"\n",
    "OUTDIR = \"out/exp1\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "# tokenize\n",
    "s1_train_tok = s1_train_ds.map(preprocess, batched=True, remove_columns= [\"text\"])\n",
    "s1_val_tok   = s1_val_ds.map(preprocess,   batched=True, remove_columns= [\"text\"])\n",
    "s1_test_tok  = s1_test_ds.map(preprocess,  batched=True, remove_columns= [\"text\"])\n",
    "s1_train_full_tok = s1_train_full_ds.map(preprocess, batched=True, remove_columns= [\"text\"])\n",
    "\n",
    "for split in (s1_test_tok, s1_train_full_tok):\n",
    "    split.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "NUM_LABELS = 2  \n",
    "counts = Counter(s1_train_full_ds[\"label\"])  \n",
    "\n",
    "\n",
    "total = sum(counts[c] for c in range(NUM_LABELS))\n",
    "inv_freq = [total / (counts[i] if counts[i] > 0 else 1) for i in range(NUM_LABELS)]\n",
    "weights = torch.tensor(inv_freq, dtype=torch.float32)\n",
    "weights = weights / weights.mean() \n",
    "print(\"class weights (0,1,2):\", weights.tolist())\n",
    "weights = weights.to(device)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ada2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(s1_train_full_ds[\"label\"], minlength=NUM_LABELS)\n",
    "w = counts.sum() / np.maximum(counts, 1)\n",
    "w = torch.tensor(w / w.mean(), dtype=torch.float32)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights  \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  \n",
    "        labels = inputs.pop(\"labels\")                \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.class_weights is None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.10\n",
    "\n",
    "if model == \"finbert\":\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=True,   \n",
    "        force_download=True,            \n",
    "    )\n",
    "elif model == \"roberta\":\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "if strategy == \"LoRA\":\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=[\"query\",\"key\",\"value\",\"dense\"]  \n",
    "    )\n",
    "    base = get_peft_model(base, lora_cfg)\n",
    "    base.print_trainable_parameters()\n",
    "\n",
    "import torch\n",
    "base.to(device)\n",
    "\n",
    "# Hyperparams (tweak if needed)\n",
    "if strategy == \"LoRA\":\n",
    "    LR = 1e-4\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"out/twitter-finance\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=max(32, BATCH_SIZE),\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        seed=8,\n",
    "        fp16=(device == \"cuda\"),\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "elif strategy == \"FT\":\n",
    "    LR = 2e-5\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"out/twitter-finance_fullft\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=max(32, BATCH_SIZE),\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=0.1,                 \n",
    "        logging_steps=50,\n",
    "        seed=8,\n",
    "        fp16=(device == \"cuda\"),\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=base,\n",
    "    args=training_args,\n",
    "    train_dataset=s1_train_full_tok,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights = w\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"global_step:\", trainer.state.global_step)\n",
    "\n",
    "\n",
    "\n",
    "test_metrics = trainer.evaluate(s1_test_tok)\n",
    "test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0df9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(s1_test_tok)  \n",
    "logits = pred.predictions            \n",
    "y_true = pred.label_ids               \n",
    "\n",
    "y_pred = logits.argmax(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_train_full_ds = s2_train_full_ds.filter(lambda ex: ex[\"label\"] in (0,1,2))\n",
    "\n",
    "\n",
    "s1_test_ds_preds = s2_test_ds.add_column(\"s1_pred\", y_pred.tolist())\n",
    "s1_test_ds_preds = s1_test_ds_preds.add_column(\"idx\", list(range(len(s1_test_ds_preds))))\n",
    "s2_test_ds_preds = s1_test_ds_preds.filter(lambda ex: ex[\"s1_pred\"] == 0)\n",
    "s2_test_ds_new = s2_test_ds_preds.remove_columns(\"s1_pred\")\n",
    "s2_test_ds_new = s2_test_ds_new.remove_columns(\"idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b384f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "\n",
    "s2_test_tok  = s2_test_ds_new.map(preprocess,  batched=True, remove_columns= [\"text\"])\n",
    "s2_train_full_tok = s2_train_full_ds.map(preprocess, batched=True, remove_columns= [\"text\"])\n",
    "\n",
    "\n",
    "for split in (s2_test_tok, s2_train_full_tok):\n",
    "    split.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "NUM_LABELS = 3  \n",
    "counts = Counter(s2_train_full_ds[\"label\"])  \n",
    "\n",
    "total = sum(counts[c] for c in range(NUM_LABELS))\n",
    "inv_freq = [total / (counts[i] if counts[i] > 0 else 1) for i in range(NUM_LABELS)]\n",
    "weights = torch.tensor(inv_freq, dtype=torch.float32)\n",
    "weights = weights / weights.mean()  \n",
    "print(\"class weights (0,1,2):\", weights.tolist())\n",
    "weights = weights.to(device)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c444d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(s2_train_full_ds[\"label\"], minlength=NUM_LABELS)\n",
    "w = counts.sum() / np.maximum(counts, 1)\n",
    "w = torch.tensor(w / w.mean(), dtype=torch.float32)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights  \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  \n",
    "        labels = inputs.pop(\"labels\")                \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.class_weights is None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.10\n",
    "\n",
    "if model == \"finbert\":\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=True,   \n",
    "        force_download=True,            \n",
    "    )\n",
    "elif model == \"roberta\":\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "if strategy == \"LoRA\":\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=[\"query\",\"key\",\"value\",\"dense\"]  \n",
    "    )\n",
    "    base = get_peft_model(base, lora_cfg)\n",
    "    base.print_trainable_parameters()\n",
    "\n",
    "\n",
    "base.to(device)\n",
    "\n",
    "if strategy == \"LoRA\":\n",
    "    LR = 1e-4\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"out/twitter-finance\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=max(32, BATCH_SIZE),\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        seed=8,\n",
    "        fp16=(device == \"cuda\"),\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "elif strategy == \"FT\":\n",
    "    LR = 2e-5\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"out/twitter-finance_fullft\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=max(32, BATCH_SIZE),\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=0.1,                \n",
    "        logging_steps=50,\n",
    "        seed=8,\n",
    "        fp16=(device == \"cuda\"),\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=base,\n",
    "    args=training_args,\n",
    "    train_dataset=s2_train_full_tok,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights = w\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"global_step:\", trainer.state.global_step)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_s2_test_tok = s2_test_tok.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(pred_s2_test_tok)  \n",
    "logits = pred.predictions             \n",
    "y_true = pred.label_ids               \n",
    "\n",
    "\n",
    "y_pred = logits.argmax(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41123a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_test_ds_preds = s2_test_ds_preds.add_column(\"s2_pred\", y_pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_fixed = s1_test_ds_preds.map(lambda ex: {\"s1_pred\": 3 if ex[\"s1_pred\"] == 1 else ex[\"s1_pred\"]})\n",
    "\n",
    "s2_lookup = dict(zip(s2_test_ds_preds[\"idx\"], s2_test_ds_preds[\"s2_pred\"]))\n",
    "\n",
    "\n",
    "def replace_with_s2(batch):\n",
    "    idxs = batch[\"idx\"]\n",
    "    s1p  = batch[\"s1_pred\"]\n",
    "    out  = []\n",
    "    for i, p in zip(idxs, s1p):\n",
    "        out.append(int(s2_lookup[i]) if i in s2_lookup else int(p))\n",
    "    return {\"s1_pred\": out}\n",
    "\n",
    "s1_merged = s1_fixed.map(replace_with_s2, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace53f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = s1_merged.add_column(\"final_pred\", s1_merged[\"s1_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([int(x) for x in final[\"label\"]], dtype=int)\n",
    "y_pred = np.array([int(x) for x in final[\"final_pred\"]], dtype=int)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "macro_f1 = f1_score(y_true, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04aa3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc, macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2,3])\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
